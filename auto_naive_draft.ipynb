{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import forecast_tools as ft\n",
    "from forecast_tools.baseline import Naive1, SNaive, baseline_estimators\n",
    "from forecast_tools.datasets import load_emergency_dept\n",
    "\n",
    "from forecast_tools.metrics import mean_absolute_error\n",
    "\n",
    "from forecast_tools.model_selection import (cross_validation_score, \n",
    "                                            rolling_forecast_origin, \n",
    "                                            sliding_window)\n",
    "\n",
    "from forecast_tools.metrics import _forecast_error_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = load_emergency_dept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_naive(y_train, horizon=1, seasonal_period=1, \n",
    "               min_train_size='auto', method='cv', step=1, \n",
    "               window_size='auto', metric='mae'):\n",
    "    '''Automatic selection of the \"best\" naive benchmark\n",
    "    \n",
    "    The selection process uses out of sample statistics.\n",
    "    \n",
    "    By default auto_naive uses cross validation to estimate the mean\n",
    "    point forecast peformance of all naive methods.  It selects the method\n",
    "    with the lowest point forecast metric on average.\n",
    "    \n",
    "    If there is limited data for training a basic holdout sample could be\n",
    "    used.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_train: array-like\n",
    "        training data.  typically in a pandas.Series, pandas.DataFrame\n",
    "        or numpy.ndarray format. \n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        Forecast horizon. \n",
    "        \n",
    "    seasonal_period: int, optional (default=1)\n",
    "        Frequency of the data.  E.g. 7 for weekly pattern, 12 for monthly\n",
    "        365 for daily.\n",
    "        \n",
    "    min_train_size: int or str, optional (default='auto')\n",
    "        The size of the initial training set (if method=='ro' or 'sw'). \n",
    "        If 'auto' then then min_train_size is set to len(y_train) // 3\n",
    "        If main_train_size='auto' and method='holdout' then \n",
    "        min_train_size = len(y_train) - horizon.\n",
    "        \n",
    "    method: str, optional (default='cv')\n",
    "        out of sample selection method. \n",
    "        'ro' - rolling forecast origin\n",
    "        'sw' - sliding window\n",
    "        'cv' - scores from both ro and sw\n",
    "        'holdout' - single train/test split\n",
    "         Methods'ro' and 'sw' are similar, however, sw has a fixed\n",
    "         window_size and drops older data from training.\n",
    "        \n",
    "    step: int, optional (default=1)\n",
    "        The stride/step of the cross-validation. I.e. the number\n",
    "        of observations to move forward between folds.\n",
    "        \n",
    "    window_size: str or int, optional (default='auto')\n",
    "        The window_size if using sliding window cross validation\n",
    "        When 'auto' and method='sw' then \n",
    "        window_size=len(y_train) // 3\n",
    "        \n",
    "    metric: str, optional (default='mae')\n",
    "        The metric to measure out of sample accuracy.\n",
    "        Options: mae, mape, smape, mse, rmse, me.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        'model': baseline.Forecast\n",
    "        'metric': float\n",
    "        \n",
    "        Contains the model and its CV performance.\n",
    "    '''\n",
    "    valid_methods = ['holdout', 'ro', 'sw', 'cv']\n",
    "    metrics = _forecast_error_functions()\n",
    "    \n",
    "    if method not in valid_methods:\n",
    "        raise ValueError(f\"Method must be in {valid_methods}\")\n",
    "        \n",
    "    if metric not in metrics:\n",
    "        raise ValueError(f\"Please select a metric from {metrics}\")\n",
    "                \n",
    "    if min_train_size == 'auto':\n",
    "        min_train_size = len(y_train) // 3\n",
    "        \n",
    "    if window_size == 'auto':\n",
    "        window_size = len(y_train) // 3\n",
    "        \n",
    "    baselines = baseline_estimators(seasonal_period)\n",
    "     \n",
    "    method_score = []\n",
    "    if method == 'cv':\n",
    "        for _, model in baselines.items():\n",
    "            cv = rolling_forecast_origin(train=y_train, \n",
    "                                         min_train_size=min_train_size,\n",
    "                                         horizon=horizon, \n",
    "                                         step=step)\n",
    "\n",
    "            score_ro = cross_validation_score(model, cv, metrics[metric])\n",
    "            \n",
    "            cv = sliding_window(train=y_train, \n",
    "                                window_size=window_size,\n",
    "                                horizon=horizon, \n",
    "                                step=step)\n",
    "\n",
    "            score_sw = cross_validation_score(model, cv, metrics[metric])\n",
    "            score = np.concatenate([score_ro, score_sw])\n",
    "            method_score.append(score.mean())\n",
    "            \n",
    "    elif method == 'ro':\n",
    "        for _, model in baselines.items():\n",
    "            cv = rolling_forecast_origin(train=y_train, \n",
    "                                         min_train_size=min_train_size,\n",
    "                                         horizon=horizon, \n",
    "                                         step=step)\n",
    "\n",
    "            score = cross_validation_score(model, cv, metrics[metric])\n",
    "            method_score.append(score.mean())\n",
    "         \n",
    "    elif method == 'sw':\n",
    "        for _, model in baselines.items():\n",
    "            cv = sliding_window(train=y_train, \n",
    "                                window_size=window_size,\n",
    "                                horizon=horizon, \n",
    "                                step=step)\n",
    "\n",
    "            score = cross_validation_score(model, cv, metrics[metric])\n",
    "            method_score.append(score.mean())\n",
    "        \n",
    "    else:\n",
    "        #single train test split\n",
    "        min_train_size = len(y_train) - horizon\n",
    "        train = y_train[:min_train_size]\n",
    "        test = y_train[min_train_size:]\n",
    "        \n",
    "        for _, model in baselines.items():\n",
    "            model.fit(train)\n",
    "            y_preds = model.predict(horizon)\n",
    "            score = mean_absolute_error(test, y_preds)\n",
    "            method_score.append(score.mean())\n",
    "            \n",
    "    method_score = np.array(method_score)\n",
    "    best_index = np.argmin(method_score)\n",
    "    \n",
    "    best = {'model':list(baselines.items())[best_index][1],\n",
    "            f'{metric}':method_score[best_index]}\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Average(), 'mae': 19.521472851717423}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = auto_naive(ed, seasonal_period=7, horizon=7, method='ro', metric='mae')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Average(), 'mae': 19.679856211931035}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = auto_naive(ed, seasonal_period=7, horizon=7, method='cv', metric='mae')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Average(), 'mae': 19.919883637665595}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = auto_naive(ed, seasonal_period=7, horizon=56, method='sw', metric='mae')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Average(), 'mae': 20.09970238095238}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = auto_naive(ed, seasonal_period=7, horizon=56, method='holdout', metric='mae')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function forecast_tools.metrics.mean_absolute_error(y_true, y_pred)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
